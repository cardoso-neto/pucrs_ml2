{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our code is generic enough to run both MNIST datasets.\n",
    "# this flag decides which one to run.\n",
    "fashion = True\n",
    "if fashion:\n",
    "    chosen_dataset = datasets.FashionMNIST\n",
    "else:\n",
    "    chosen_dataset = datasets.MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(loader):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    for images, _ in loader:\n",
    "        # batch size (the last batch can have smaller size!)\n",
    "        batch_samples = images.size(0) \n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "\n",
    "    mean /= len(loader.dataset)\n",
    "    std /= len(loader.dataset)\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "def get_loaders(batch_size):\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.28604069352149963,), (0.32045337557792664,)), # fash\n",
    "        # transforms.Normalize((0.13066062331199646,), (0.301504522562027,)), # mnist\n",
    "    ])\n",
    "\n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.28604069352149963,), (0.32045337557792664,)), # fash\n",
    "        # transforms.Normalize((0.13066062331199646,), (0.301504522562027,)), # mnist\n",
    "    ])\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=chosen_dataset(\n",
    "            root='../data', \n",
    "            train=True, \n",
    "            download=True,\n",
    "            transform=train_transforms,\n",
    "        ),\n",
    "        batch_size=batch_size, \n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=chosen_dataset(\n",
    "            root='../data', \n",
    "            train=False, \n",
    "            download=True,\n",
    "            transform=test_transforms,\n",
    "        ),\n",
    "        batch_size=batch_size, \n",
    "        shuffle=False\n",
    "    )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def train_epoch(\n",
    "    model, \n",
    "    device, \n",
    "    train_loader, \n",
    "    optimizer, \n",
    "    criterion, \n",
    "    epoch, \n",
    "    log_interval\n",
    "):\n",
    "    model.train()\n",
    "    history = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch,\n",
    "                batch_idx * len(data),\n",
    "                len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item()\n",
    "            ))\n",
    "\n",
    "\n",
    "def test(\n",
    "    model, \n",
    "    device, \n",
    "    criterion, \n",
    "    test_loader\n",
    "):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.max(dim=1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss,\n",
    "        correct,\n",
    "        len(test_loader.dataset),\n",
    "        accuracy\n",
    "    ))\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    device,\n",
    "    lr,\n",
    "    nb_epochs=3,\n",
    "    log_interval=100,\n",
    "):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    for epoch in range(1, nb_epochs + 1):\n",
    "        print('\\n* * * Training * * *')\n",
    "        train_epoch(\n",
    "            model=model, \n",
    "            device=device, \n",
    "            train_loader=train_loader, \n",
    "            optimizer=optimizer, \n",
    "            criterion=criterion, \n",
    "            epoch=epoch, \n",
    "            log_interval=log_interval\n",
    "        )\n",
    "        print('\\n* * * Evaluating * * *')\n",
    "        acc = test(model, device, criterion, test_loader)        \n",
    "    \n",
    "    return acc\n",
    "\n",
    "def check_input(model, device):\n",
    "    \"\"\"\n",
    "    Make sure the model outputs a correctly shaped tensor\n",
    "    which is (batch_size, number_of_classes)\n",
    "    \"\"\"\n",
    "    dummy_data = torch.zeros(5, 1, 28, 28).to(device)\n",
    "    dummy_pred = model(dummy_data)\n",
    "    error_message = (\n",
    "        '\\nOutput expected: (batch_size, 10)'\n",
    "        f'\\nOutput found   : {dummy_pred.shape}'\n",
    "    )\n",
    "    assert dummy_pred.shape == (5, 10), error_message\n",
    "    print('Passed.')\n",
    "    return dummy_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "device_name = 'cuda'\n",
    "nb_epochs = 3\n",
    "log_interval = 500\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(device_name)\n",
    "train_loader, test_loader = get_loaders(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  torch.Size([60000, 28, 28]) torch.Size([60000])\n",
      "Test size :  torch.Size([10000, 28, 28]) torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    'Train size: ', \n",
    "    train_loader.dataset.data.shape, \n",
    "    train_loader.dataset.targets.shape,\n",
    ")\n",
    "print(\n",
    "    'Test size : ', \n",
    "    test_loader.dataset.data.shape, \n",
    "    test_loader.dataset.targets.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes = [\n",
    "    'T-shirt/top',\n",
    "    'Trouser',\n",
    "    'Pullover',\n",
    "    'Dress',\n",
    "    'Coat',\n",
    "    'Sandal',\n",
    "    'Shirt',\n",
    "    'Sneaker',\n",
    "    'Bag',\n",
    "    'Ankle Boot'\n",
    "]\n",
    "pics = random.sample(range(len(train_loader)), 5)\n",
    "fig, axes = plt.subplots(1, 5)\n",
    "for i, axis in zip(pics, axes):\n",
    "    axis.imshow(train_loader.dataset.train_data[i], cmap='gray')\n",
    "    title = train_loader.dataset.train_labels[i].item()\n",
    "    if fashion:\n",
    "        title = classes[title]\n",
    "    axis.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance Example: torch.Size([32, 1, 28, 28]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "instance: Tuple[torch.Tensor, torch.Tensor] = next(iter(train_loader))\n",
    "print('Instance Example:', instance[0].shape, instance[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron\n",
    "\n",
    "Three fully connected layers:\n",
    "A layer 784 outputs (28 * 28, the size of the image),\n",
    "a hidden layer with 350 outputs; roughly half the previous layer so we can start downsapling our \"image\",\n",
    "and the final layer with 10 outputs that are our classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed.\n",
      "MLPNet(\n",
      "  (lin1): Linear(in_features=784, out_features=784, bias=True)\n",
      "  (lin2): Linear(in_features=784, out_features=350, bias=True)\n",
      "  (fc): Linear(in_features=350, out_features=10, bias=True)\n",
      "  (relu): ReLU(inplace)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(in_features=784, out_features=784)\n",
    "        self.lin2 = nn.Linear(in_features=784, out_features=350)\n",
    "        self.fc = nn.Linear(in_features=350, out_features=10)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], 28 * 28)\n",
    "        x = self.lin1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.relu(x)\n",
    "        out = self.fc(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "mlpnet = MLPNet().to(device)\n",
    "dummy_pred = check_input(mlpnet, device)\n",
    "print(mlpnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* * * Training * * *\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.321942\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.393759\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.191554\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.212495\n",
      "\n",
      "* * * Evaluating * * *\n",
      "Test set: Average loss: 0.0126, Accuracy: 8560/10000 (85.60%)\n",
      "\n",
      "\n",
      "* * * Training * * *\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.445318\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.401945\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.546911\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.208726\n",
      "\n",
      "* * * Evaluating * * *\n",
      "Test set: Average loss: 0.0125, Accuracy: 8585/10000 (85.85%)\n",
      "\n",
      "\n",
      "* * * Training * * *\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.402490\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.274388\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.229208\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.454045\n",
      "\n",
      "* * * Evaluating * * *\n",
      "Test set: Average loss: 0.0124, Accuracy: 8596/10000 (85.96%)\n",
      "\n",
      "Final acc: 85.96%\n"
     ]
    }
   ],
   "source": [
    "acc = train(mlpnet, train_loader, test_loader, device, 1e-4, nb_epochs, log_interval)\n",
    "print(f'Final acc: {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Inception layer, but with fully connected layers.\n",
    "Inspired by the now world-famous paper by Google employees (InceptionV1)[https://arxiv.org/pdf/1409.4842.pdf] that uses parallel convolutions to make better use of the immense parallelization capabilities GPUs provide to start growing neural networks girth-wise instead of only depth-wise, we created this LinearInception module.\n",
    "We thought this would give our model a bigger receptive field and that that would give us a huge gain in accuracy, but we were wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed.\n",
      "InceptionMLPNet(\n",
      "  (lin1): Linear(in_features=784, out_features=784, bias=True)\n",
      "  (lin2): LinearInception(\n",
      "    (fcs): ModuleList(\n",
      "      (0): Linear(in_features=784, out_features=350, bias=True)\n",
      "      (1): Linear(in_features=784, out_features=350, bias=True)\n",
      "      (2): Linear(in_features=784, out_features=350, bias=True)\n",
      "    )\n",
      "    (fc_aggregate): Linear(in_features=1050, out_features=350, bias=True)\n",
      "  )\n",
      "  (lin_end): Linear(in_features=350, out_features=10, bias=True)\n",
      "  (relu): ReLU(inplace)\n",
      "  (dropout): Dropout(p=0.1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LinearInception(nn.Module):\n",
    "    def __init__(self, in_features, out_features, cardinality):\n",
    "        super().__init__()\n",
    "        self.card = cardinality\n",
    "        self.fcs = nn.ModuleList([\n",
    "            nn.Linear(in_features=in_features, out_features=out_features)\n",
    "            for _ in range(cardinality)\n",
    "        ])\n",
    "        self.fc_aggregate = nn.Linear(\n",
    "            in_features=out_features * cardinality,\n",
    "            out_features=out_features\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.parallel.parallel_apply(self.fcs, (x,) * self.card)\n",
    "        x = torch.stack(x, dim=1)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc_aggregate(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class InceptionMLPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(in_features=784, out_features=784)\n",
    "        self.lin2 = LinearInception(in_features=784, out_features=350, cardinality=3)\n",
    "        self.lin_end = nn.Linear(in_features=350, out_features=10)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], 28 * 28)\n",
    "        x = self.lin1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        out = self.lin_end(x)\n",
    "        return out\n",
    "\n",
    "incmlp = InceptionMLPNet().to(device)\n",
    "dummy_pred = check_input(incmlp, device)\n",
    "print(incmlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* * * Training * * *\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.450436\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.292167\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.424795\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.604289\n",
      "\n",
      "* * * Evaluating * * *\n",
      "Test set: Average loss: 0.0126, Accuracy: 8542/10000 (85.42%)\n",
      "\n",
      "\n",
      "* * * Training * * *\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.613697\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.315931\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.192128\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.341053\n",
      "\n",
      "* * * Evaluating * * *\n",
      "Test set: Average loss: 0.0124, Accuracy: 8574/10000 (85.74%)\n",
      "\n",
      "\n",
      "* * * Training * * *\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.190846\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.079334\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.363328\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.349266\n",
      "\n",
      "* * * Evaluating * * *\n",
      "Test set: Average loss: 0.0123, Accuracy: 8578/10000 (85.78%)\n",
      "\n",
      "Final acc: 85.78%\n"
     ]
    }
   ],
   "source": [
    "acc = train(incmlp, train_loader, test_loader, device, 1e-4, nb_epochs, log_interval)\n",
    "print(f'Final acc: {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNet só para não perder o costume.\n",
    "\n",
    "Duas convs e uma fully connected layer no final para dizer minhas classes.\n",
    "Depois de cada conv, uma relu para introduzir não-linearidade e um MaxPooling para reduzir o tamanho conforme a dimensionalidade aumenta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed.\n",
      "ConvNet(\n",
      "  (conv_1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc): Linear(in_features=3136, out_features=10, bias=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU(inplace)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv_2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(in_features=3136, out_features=10)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.shape[0], 64 * 7 * 7) # Ou x = x.view(x.shape[0], -1)\n",
    "        out = self.fc(x)\n",
    "        return out\n",
    "    \n",
    "convnet = ConvNet().to(device)\n",
    "dummy_pred = check_input(convnet, device)\n",
    "print(convnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* * * Training * * *\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.353777\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.151119\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.399603\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.368347\n",
      "\n",
      "* * * Evaluating * * *\n",
      "Test set: Average loss: 0.0105, Accuracy: 8813/10000 (88.13%)\n",
      "\n",
      "\n",
      "* * * Training * * *\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.293477\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.235461\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.311738\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.206354\n",
      "\n",
      "* * * Evaluating * * *\n",
      "Test set: Average loss: 0.0106, Accuracy: 8808/10000 (88.08%)\n",
      "\n",
      "\n",
      "* * * Training * * *\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.558048\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.381771\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.115153\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.386943\n",
      "\n",
      "* * * Evaluating * * *\n",
      "Test set: Average loss: 0.0104, Accuracy: 8841/10000 (88.41%)\n",
      "\n",
      "Final acc: 88.41%\n"
     ]
    }
   ],
   "source": [
    "acc = train(convnet, train_loader, test_loader, device, 1e-4, nb_epochs, log_interval)\n",
    "print(f'Final acc: {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observações\n",
    "Embora saibamos que MPLs são inapropriados para tarefas de imagem, porque cada neorônio fica atrelado a um pixel o que as não permite que o modelo se torne invariante a translações, redimensionamentos, etc., decidimos usá-los mesmo assim, pois os conjuntos de dados que escolhemos (MNIST e FashionMNIST) contêm apenas imagens bem centralizadas e de dimensões muito semelhantes entre si.\n",
    "\n",
    "Nosso melhor resultado foi no MLP tradicional com learning rate de 0.001 por três epochs. Dividir a learning rate por 10 e treinar mais dois epochs conseguiu um ganho de aproximadamente 1% e é o que fizemos para todos modelos.\n",
    "\n",
    "### State of the art\n",
    "Estado da arte (sem deep learning) do dataset que escolhemos é 89.7% accuracy e o modelo usado é um SVC (roda em tempo quadrático em relação ao número de instâncias) e foi treinado por 1h12min (mais detalhes http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/), enquanto nossos modelos todos demoram menos de 1 minuto para serem treinados.\n",
    "\n",
    "Estado da arte atual do dataset é do github user (@ajbrock)[https://github.com/ajbrock] com 96.7% de accuracy usando uma (wide residual network)[https://arxiv.org/pdf/1605.07146.pdf] de 40 camadas e largura 4 (8,9M paramêtros).\n",
    "\n",
    "### Our results\n",
    "Our data augmentation steps are just standard preprocessing (mean/std subtraction/division) and random horizontal flips.\n",
    "\n",
    "Acc: 85.96% na MLPNet.  Hyperparams: 3 epochs 1e-3 + 3 epochs 1e-4.\n",
    "Acc: 85.78% na IncMLP.  Hyperparams: 3 epochs 1e-3 + 3 epochs 1e-4.\n",
    "Acc: 88.41% na ConvNet. Hyperparams: 3 epochs 1e-3 + 3 epochs 1e-4.\n",
    "\n",
    "### Conclusão\n",
    "Mais epochs só fizeram nosso modelo overfit, então provavelmente estamos no limite da capacidade desses modelos (ignorando possíveis estratégias de regularização).\n",
    "\n",
    "A partir dos nossos testes empíricos, notamos que mesmo com modificações grandes, que supostamente deveriam tornar o modelo muito mais robusto, conseguimos pouquíssimos ganhos de accuracy.\n",
    "\n",
    "Na nossa opinião, isso significa que estamos nos aproximando do ponto em que a derivada da curva de ganhos em relação a esforços é quase zero, o que significa que precisamos de muito, muito mais esforços (por exemplo o state of the art com quase 9 milhões de parâmetros e nossa MPLNet com 893700, ele precisou de 10 vezes mais parâmtetros e uma estrutura muito mais complicada para 9% de melhoria)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
